{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "d1b8a4b2c9504039b8eaee12af61420c",
      "ed143e057fe34cbead8ac9f259751364",
      "33dd9e4d643640f0b7e312eb51fde1e3",
      "d2377ab335854ccca9f65695dc8b0978",
      "0a1855b8ca574c2093ea5f0511610b22",
      "46f9aaabd0b04a1c964d9ec9dce7572d",
      "7d0515d9484b41ba8e5d4466fb80d39c",
      "ea67310111674b1cade68f3e43474191",
      "d49b0ec180344bfc898aa611578d2bb6",
      "d91719ece7f24b1dbe5124b5caf25b56",
      "353f3a532c83473c84dcf830881d8cf5",
      "41f4b3a9bd8d4183959287e11eb8e2d9",
      "9214209d2aa34648826249e9f9ec04cf",
      "4d3e2ef10eb44933bdebac7bd8946a7f",
      "9ae1ffa649b74c0c9179a3ac3fb25289",
      "6584b349756e448bb03b00b65e683041",
      "71e398ddc3bf4761bf7e3f0b58020298",
      "50562f042ae4474b962c74dad80977fc",
      "ffe02b764d5f4268beed13c04b636ada",
      "f9b47502c8b8482e88c6f5ea0222d5f2",
      "448308e4944f468185d158b91ae4f5a6",
      "29233cf7007f457887c256288040b1ff",
      "ac889b91e96e4af9bdf5bb5d2298d93a",
      "ddcf4520f38140f9a86244bdd5e3c10f",
      "6f69370dabd84e7382db51b6b9bdfd17",
      "3cad772874d440eda9b359cc15d221c2",
      "759861bcd16a43ef936d285e4bb6f2ea",
      "ec59514d5d964625971b282ba2db0b85",
      "8d2ba7026560468393ce99c1df3b8d73",
      "e2e683b2fbe84e23bee3be3726d4456e",
      "44f15908b4bc4cbd96eb34f04c64396b",
      "b5c22f001cde4057925c31212cc3a680",
      "00b31ef7ef2a43f98337d7d1a8f38b2a",
      "b45fb0dbd6714be2baeb39d8470c79b7",
      "a1909063ce62441393e761441d28a4bf",
      "e25a2a3cd8054d2d96d0232c5f5610f6",
      "94b5ec93efd7439588d61997997b9f3c",
      "5662858f3891440bb04561317f2c7b3d",
      "42980c11e839431dbee9e638ed180719",
      "2b863b184c354aed902b1c2382e5305b",
      "089592385aab4ccd84173309294dc32e",
      "1768db4a199f484eb352bd921be02ec0",
      "03dde29053a6472dbc07a7dd54ec43bd",
      "b579139165444d08ae155497a0951400",
      "e2d1ca8251b046aca4b551289a6f1a10",
      "b48f5d7f45624d23ac27016c39bbb036",
      "fe9dc6d31b354c5cbd15b613301c604b",
      "d6205963da7048c9a543ef6bdebd6024",
      "dd67d9d81fe840f88b930ce357192d8b",
      "0f00dd3031d54b18b2e5b800973f639a",
      "7a7337e1583540e0b3db325b1822454b",
      "dc6277deaefa4dd4b323ad837442be76",
      "dd8c1350c0094b4c8a1797eb0b65e580",
      "bd8c5ec51f7744eaa137923ab5557d89",
      "c938867b442244a4b294446d3f4ffd33",
      "62ee57a88bc0426dbe8916d15c30d107",
      "ac91e70e96aa4ce1915c64f53cf95d0a",
      "bfe62b8ee6e7461cb9928f5cf74b2b50",
      "4ad7b762f0dc4f0e93d8715e8a88aadf",
      "0b439b9bb4714a69b92a52fcbc7139a4",
      "6cfb745011114006a07800d55449a211",
      "e5c21d975c9743d899ed5b47a91a52f0",
      "771233f8fb2c40fc8dec6f6607fef711",
      "304cb7bf70614369acf831a93c5428e5",
      "5f9d106e452b4f25917abe9ed0908aee",
      "fe06e65c10b64cb09b12ae4b262ba9a4",
      "69c793e7dbc24927aed5ed10e2d3472b",
      "2e6021991cc847d899742c8b07ae6d52",
      "5e6f1c7e93754be5acdf8519da23ef5b",
      "a22643936057433d9c679ab674566d49",
      "60b6aa15f8cd448486f463547e529534",
      "8970094f9b49477fa37adcc261d6c58c",
      "f9a84a17901d4aa2902aec87c0f13219",
      "f2866b7f7e45493fa1ba9ebe64405250",
      "20002e12275c45e1ada12c6ddd9bb662",
      "a4addc8ef35b4fdca72baa93bb8c2b7b",
      "3e4a4f1f78c641a593576436e3917268",
      "2dff79d9b9a4414391965f920d7d966f",
      "8c23cc0f0c774537b133c162278ab32a",
      "d1ab2a266f534fd1b5a1fb7dc8342109",
      "8b5721fad1f046c68fef6b850c1a23e3",
      "5ca9ed42dc7c46f5b25b8bf4401ba880",
      "c00da3bb90f945ccbb4241607fac0954",
      "0295fb5e959d4ebab958b2db06860091",
      "c2fef8b145e8427d900909e306371d47",
      "a848d49332f947878fb860d1e73a1830",
      "41c0c5ef8f4948c4819bcb0a14592318",
      "3c8eff9875b04032ab521596a8bdfbc2",
      "5e7d629e0dcb4ebd894c166e52c13c6d",
      "7b79a214420944e7beb5652331459dfb",
      "242a4e098f1244cb9184f575f9c81fb5",
      "c602bc0e03484409b0eaf8bae821fb74",
      "742467bfd1cb490e85acbfe75813393d",
      "26bb11002d32464c8a2825280bcbc340",
      "129806e4351d406f9a6d5b3ed0db96a7",
      "c304d8c190e24c479896e013f247fd9f",
      "6d18d553ffbe461da2e8b56082bad416",
      "c3c9b940c9c64d3fb98a51dac6dbe6bd",
      "7cd860dfcf624b05ba13099ee09a8694",
      "63e48b80388e410e9cf35e9a7ae7f495",
      "7f5a98dde10a4b549928446269ef7a6b",
      "eb957ff651374713883e0eb90ec0bd4a",
      "a2e4373d96d94e2d9fe2726ae05fc577",
      "7e93bd54a0e64d6092803055431a4c7d",
      "b6ff23f37b7348ac98ba12ab1c8feead",
      "ee2cebbfd12b4ad880d41dadc19d3142",
      "bdd955a48a0a4595b8c1b5aaad63ac15",
      "10cd34fef53d470498559e5f43cd97b5",
      "a53a58311080438eb8f21605a25d0416",
      "15f8f85c806a4304af999355a8515cdc",
      "c93597a408204376bac4909cdb3f9c28",
      "7c688870c21a4c119219399970b9eb43",
      "89f7ad7e821f4a2d993acdc77565eeeb",
      "60768571bfe6497b8114915ceadda288",
      "5378b6029f654f24a869573298be8513",
      "d9776e90793544d2815aca0633619228",
      "e8e9edc0ece544f9aef067d5395bb5bc",
      "7f2ec913167745549224ccab84c1f2f3",
      "ef03fd08015d4c4eb8586916107ea3a0",
      "706e0b70c28b4478806f9c40f986fc3c",
      "e7680aa396494bd6bfd0fb7069253496"
     ]
    },
    "id": "a5988dNEWCMW",
    "outputId": "aa03abf9-2e61-4167-e567-a1ed9b7fa62e"
   },
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "from IPython.display import display\n",
    "\n",
    "# Installer les d√©pendances n√©cessaires\n",
    "!pip install -U langgraph langsmith langchain_anthropic\n",
    "!pip install -U openai\n",
    "!pip install -U transformers huggingface_hub\n",
    "!pip install torch\n",
    "\n",
    "import os\n",
    "\n",
    "# Charger le token Hugging Face depuis .env\n",
    "from google.colab import files\n",
    "uploaded = files.upload()  # T√©l√©chargez votre fichier .env\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM  # Import manquant\n",
    "\n",
    "load_dotenv(\".env\")  # Charge votre token\n",
    "token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "print(\"‚úÖ Token charg√© :\", token[:10] + \"...\")\n",
    "\n",
    "# Charger le mod√®le Mistral\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "# Charger le tokenizer et le mod√®le avec float16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    token=token,\n",
    "    torch_dtype=torch.float16,  # Forcer FP16 pour r√©duire la m√©moire\n",
    "    device_map=\"auto\"  # GPU si disponible\n",
    ")\n",
    "\n",
    "# V√©rifier le chargement du mod√®le\n",
    "print(\"‚úÖ Mod√®le charg√© sur :\", model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HqlMIBYlr804",
    "outputId": "6662c4bb-2fd9-4791-e3de-e1bb624c1fe0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wnaSbPyxXEr7",
    "outputId": "3715230a-3699-40b0-81be-a1dc472f7ac6"
   },
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from transformers import AutoTokenizer\n",
    "import textwrap\n",
    "import random\n",
    "\n",
    "prompts = [\n",
    "    \"Quels sont les principaux avantages de l'IA pour la soci√©t√©?\",\n",
    "    \"Quels sont les dangers potentiels de l'IA?\",\n",
    "    \"Comment l'IA peut-elle √™tre utilis√©e pour r√©soudre des probl√®mes mondiaux?\",\n",
    "    \"L'IA est-elle une menace pour l'emploi?\",\n",
    "    \"Comment garantir une utilisation √©thique de l'IA?\",\n",
    "]\n",
    "\n",
    "# D√©finition de l'√©tat du d√©bat\n",
    "class DebateState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]  # Liste des messages √©chang√©s\n",
    "\n",
    "# Cr√©ation du graphe\n",
    "graph_builder = StateGraph(DebateState)\n",
    "\n",
    "def agent1(state: DebateState):\n",
    "    \"\"\"L'agent 1 donne son point de vue\"\"\"\n",
    "    last_message = state[\"messages\"][-1].content\n",
    "    # Extraire le prompt al√©atoire du dernier message\n",
    "    last_prompt = last_message.split(\" \")[-1]\n",
    "\n",
    "    # Formuler l'input pour le mod√®le en fonction du prompt\n",
    "    input_text = f\"## Argument en faveur de l'IA concernant: {last_prompt}\\n\"  # D√©but de l'argument\n",
    "\n",
    "    # ... (Vous pouvez ajouter des instructions sp√©cifiques ici)\n",
    "    input_text += \"Veuillez d√©velopper un argument en faveur des b√©n√©fices de l'IA en vous concentrant sur ce point.\"\n",
    "\n",
    "    # D√©finir 'inputs'\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_new_tokens=150)\n",
    "\n",
    "    response_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    response_text = textwrap.fill(response_text, width=80)\n",
    "    return {\"messages\": state[\"messages\"] + [AIMessage(content=response_text, type='ai')]}\n",
    "\n",
    "def agent2(state: DebateState):\n",
    "    \"\"\"L'agent 2 r√©agit √† l'argument de l'agent 1\"\"\"\n",
    "    last_message = state[\"messages\"][-1].content\n",
    "    last_prompt = last_message.split(\" \")[-1]  # Supposant que le prompt est le dernier mot\n",
    "\n",
    "    # Formuler l'input pour le mod√®le en fonction du prompt\n",
    "    input_text = f\"## Argument contre l'IA concernant: {last_prompt}\\n\"\n",
    "\n",
    "    # ... (Vous pouvez ajouter des instructions sp√©cifiques ici)\n",
    "    input_text += \"Veuillez d√©velopper un argument contre les dangers de l'IA en vous concentrant sur ce point.\"\n",
    "\n",
    "    # D√©finir 'inputs'\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_new_tokens=150, temperature=0.8)\n",
    "\n",
    "    response_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Ajuster le retour √† la ligne du texte\n",
    "    response_text = textwrap.fill(response_text, width=80)  # Ajuster la largeur selon vos besoins\n",
    "    # Retourner un AIMessage\n",
    "    return {\"messages\": state[\"messages\"] + [AIMessage(content=response_text, type='ai')]}\n",
    "\n",
    "# Ajouter les agents dans le graphe\n",
    "graph_builder.add_node(\"agent1\", agent1)\n",
    "graph_builder.add_node(\"agent2\", agent2)\n",
    "\n",
    "# Lancement du d√©bat par l'agent 1\n",
    "graph_builder.add_edge(START, \"agent1\")\n",
    "# L'agent 1 passe la main √† l'agent 2\n",
    "graph_builder.add_edge(\"agent1\", \"agent2\")\n",
    "# L'agent 2 renvoie √† l'agent 1 pour continuer le d√©bat\n",
    "graph_builder.add_edge(\"agent2\", \"agent1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sOQxCcLYXPLK",
    "outputId": "016af9fc-b1cf-4caf-8640-5a9ef970ca5f"
   },
   "outputs": [],
   "source": [
    "# Compiler le graphe\n",
    "debate_graph = graph_builder.compile()\n",
    "\n",
    "# Fonction pour simuler le d√©bat\n",
    "def run_debate(initial_prompt, num_turns=5):\n",
    "    # Initialiser avec HumanMessage\n",
    "    state = {\"messages\": [HumanMessage(content=initial_prompt)]}\n",
    "\n",
    "    for i, event in enumerate(debate_graph.stream(state)):\n",
    "       # Choisir un prompt al√©atoire pour ce tour\n",
    "        current_prompt = random.choice(prompts)\n",
    "\n",
    "        # Ajouter le prompt au message du tour actuel\n",
    "        state['messages'][-1].content += \" \" + current_prompt # on ajoute le prompt current au dernier prompt existant\n",
    "\n",
    "        for value in event.values():\n",
    "            # Convertir le dernier message en AIMessage ou HumanMessage\n",
    "            last_message = value['messages'][-1]\n",
    "            # Acc√©der au type du message avec .type\n",
    "            if last_message.type in (\"agent1\", \"agent2\"):\n",
    "                message = AIMessage(content=last_message.content)\n",
    "            else:\n",
    "                message = HumanMessage(content=last_message.content)\n",
    "\n",
    "            print(f\"üó£Ô∏è Tour {i+1} - {message.type} : {message.content}\")\n",
    "\n",
    "        if i >= num_turns - 1:\n",
    "            break\n",
    "\n",
    "\n",
    "# Lancer le d√©bat avec un sujet donn√©\n",
    "run_debate(\"L'intelligence artificielle est-elle b√©n√©fique ou dangereuse ?\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
